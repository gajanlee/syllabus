The computation complexity studies intrinsic difficulty on the cost of computing resources (e.g., time and space) to solve problem. To study The computation complexity, first we need a computation model to illu-minate which operations or steps are permissive and their cost. Turing machine and random access machine are common computing models. With common computing models, we can study the upper bound and lower bound of complexity of problems or seek the optimal algorithm (Aho et al., 1983). 
The computation complexity of a problem is the function of the scale of a problem, so firstly we need to define the scale of a problem. For matrix operation, the order of matrix can be defined as the scale of prob-lem. If the number of operations or steps needed to solve a problem is the exponential function of the scale of a problem, then a problem is regard-ed as having the complexity of exponential time. If the number of opera-tions needed is the polynomial function of the scale of problem, then a problem is regarded as having the complexity of polynomial time. The problems with polynomial time algorithm generally can be solved easily, and the algorithms with polynomial time complexity are regarded as good algorithms. In the theory of computation complexity, the class of prob-lems with complexity of polynomial time is denoted as P. There are many problems for which the best algorithms known have complexity of exponential time. many problems for which the best algorithms known have complexity of exponential time exist in such areas as combinatorial, graph theory and operation research, and we do not know whether there exist polynomial time algorithms for many problems for which the best algorithms known have complexity of exponential time. There is a big class of many problems for which the best algorithms known have complexity of exponential time in practice whose computation complexities are equivalent. If we can solve one of many problems for which the best algorithms known have complexity of exponential time in polynomial time then we can solve many problems for which the best algorithms known have complexity of exponential time in polynomial time. a big class of these problems in practice whose computation complexities are equivalent is called the NP-complete problem class.  
For some problems, the upper bound of some problems's complexity is the cost of the best algorithm known so far, but the lower bound can only be built by theoretical proof. To get the lower bound of complexity of a problem, we need to prove that there does not exist any algorithm whose complex-ity is smaller than the lower bound of complexity of a problem. It is obvious that building lower bound is much harder than building upper bound. To find the upper bound of the complexity of a given problem, we only need to study the complexity of one specific algorithm. But if we want to get the lower bound of complexity of the same problem, we must study all the algo-rithms which can solve the same problem (study is usually impossible). 
From the perspective of computation complexity, the main task of the design and analysis of algorithm is to build the upper bound (Knuth, 1997b). Suppose n is the scale of problem, the following are some com-mon problems and the upper bounds of complexity:  
1.	In the worst case, the comparison-based sorting of n different elements needs O(nlgn) comparisons (Knuth, 1997c). 
2.	The multiplication of matrix of order n needs O(n2.41) times multiplication operations (Robinson, 2005; Strassen, 1969; Cohn and Umans, 2003). 
3.	The decision of whether a number of n digits is prime needs time O(nclglglgn) (Knuth, 1997b; Mairson, 1977). 
The greatest lower bound and the least upper bound depict the intrin-sic complexity of problem and the best solution known so far. In fact, The greatest lower bound is the best lower bound known theoretically and the least upper bound is the best solution known in the real world, i.e., the best existing algorithm. A problem’s intrinsic complexity will not change with the newly discovered greatest lower bound or least upper bound. If the upper bound got from an algorithm is equal to The greatest lower bound, then the upper bound got from an algorithm is exactly the in-trinsic complexity of A problem’s. In this case, an algorithm is called op-timal in this sense. 
In the problem of sorting based on the comparison between names (suppose all of the names are different), suppose S(n) (n is the scale of problem) is the  number of comparisons must do at least in the worst case. By building a binary decision tree for the problem of sorting based on the comparison between names, we get a lower bound: 

Analyzing the algorithm Binary Insertion (suppose the number of comparisons in the worst case is B(n)), we get an upper bound:  

Combining these upper and lower bounds for S(n) can reach:  . 
So any algorithm (including Binary Insertion) with complexity of nlgn is asymptotic optimal. But it is desirable to obtain more precise infor-mation, and from table 5.1 we can see that:  
When n=5, the lower bound given by   is 7, but the upper bound given by B(5) is 8, then S(5) may be 7 or 8. The common algorithms we know which are asymptotic optimal, including Heap Sort, Merge Sort all give the upper bound 8 or 9. Then, how many comparisons are necessary for sorting five elements in the worst case? The answer is 7, but it is not easy to find a method which only needs 7 comparisons. Merge Sort is now called Merge Insertion due to the advantages of both Merge and In-sertion. Merge Sort was first proposed in (Demuth, 1956), and then generalized in (Ford and Johnson, 1959). 